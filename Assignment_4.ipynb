{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: kNN, Naive Bayes and Text Feature Selection\n",
    "\n",
    "#### Full Name: April Hudspeth\n",
    "#### Student ID: 995032557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we'll explore kNN and Naive Bayes further and look at different feature selection strategies. First of all, download the dataset for this assignment from Canvas and unpack into the A4 directory. \n",
    "\n",
    "The dataset is from the RCV1 v2 corpus which is a collection of news articles and category labels. There are 103 categories, and each document can lie in one or more categories. \n",
    "\n",
    "The sparse train and test matrices are encoded as nnz x 3 integer matrices, where nnz is the total number of words in all docs. Each row of these matrices contains (doc, word, count) triples. We will store those in sparse matrices whose dimensions are ndocs x nwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iwords = np.loadtxt(\"data/words.imat.txt\")          # training data matrix in nnz x 3 form - rows are (doc, word, count) triples\n",
    "cats = np.loadtxt(\"data/cats.imat.txt\")             # training labels in an ndocs x ncats matrix\n",
    "tiwords = np.loadtxt(\"data/testwords.imat.txt\")     # test data matrix in nnz x 3 form\n",
    "tcats = np.loadtxt(\"data/testcats.imat.txt\")        # testing labels in an ndocs x ncats matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iwords and tiwords encode the sparse train and test matrices. Each row of these matrices contains (doc, word, count) triples. We will store those in sparse matrices whose dimensions are ndocs x nwords. The exact matrix type is CSR which stands for compressed sparse rows. Look up its description here: https://en.wikipedia.org/wiki/Sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801667,)\n",
      "(801667,)\n",
      "(801667,)\n"
     ]
    }
   ],
   "source": [
    "fwords = iwords[:,2]\n",
    "print fwords.shape\n",
    "print iwords[:,1].shape\n",
    "print iwords[:,0].shape\n",
    "train = sp.csr_matrix((fwords, (iwords[:,1], iwords[:,0])))\n",
    "\n",
    "ndocs = train.shape[0]\n",
    "nwords = train.shape[1]\n",
    "\n",
    "test = sp.csr_matrix((tiwords[:,2], (tiwords[:,1], tiwords[:,0])),shape=(4000,nwords))  # need to match the number of cols (words)\n",
    "ntdocs = test.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the dataset has 103 category labels, we'll focus on training a single category. We will use category 6, which is fairly evenly balanced between positives and negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat6 = cats[:,6]\n",
    "tcat6 = tcats[:,6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll train Scikit-Learn's builtin kNN classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnc = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then \"fit\" it to the data. Notice how fast this is! Of course, training kNN is basically a no-op, and predicting is where all the work lies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           n_neighbors=3, p=2, weights='uniform')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnc.fit(train, cat6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try predicting. This will actually take some time, but hopefully only a few tens of seconds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = knnc.predict(test);preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the accuracy on the actual test set labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82225000000000004"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but kNN is very sensitive to the distance function as we argued in class. Let's try first normalizing each row of \"train\" and \"test\" by the L2-norm of that row, i.e. we will divide by the sqrt of the sum of the squared word counts in the row. Comparing the normalized documents should do much better. \n",
    "\n",
    "> Q1: Implement row normalization below. Compute the row norms first. Your implementation strategy is important for this to run in reasonable time. **Don't** try to construct an ndocs x ndocs dense matrix. You probably also don't want to iterate over all the docs or all the words. The fastest solution is to pre-multiply by a sparse diagonal matrix which contains the scale factors. No loops are necessary. Normalization should take a fraction of a second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 31331)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from numpy.linalg import inv\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "# row_sums = np.array(train.sum(axis=1))[:,0]\n",
    "# row_indices, col_indices = train.nonzero()\n",
    "\n",
    "\n",
    "trainnorm = train.multiply(train).sum(1)\n",
    "testnorm = test.multiply(test).sum(1)\n",
    "trainfact = 1.0 / np.sqrt(np.max(trainnorm,1))\n",
    "testfact = 1.0 / np.sqrt(np.max(testnorm,1));trainfact.shape\n",
    "trainnmat = sp.spdiags(np.transpose(trainfact), 0, ndocs, ndocs) * train\n",
    "testnmat = sp.spdiags(np.transpose(testfact), 0, ntdocs, ntdocs) * test\n",
    "testnmat.shape # should be (10000,44718)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train and evaluate a knn model using the normalized datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90275000000000005"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnc.fit(trainnmat, cat6)\n",
    "preds = knnc.predict(testnmat);preds\n",
    "np.mean(tcat6 == preds)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q2: What is the accuracy of your kNN classifier on the normalized data? **0.903**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Multinomial Naive Bayes classifier. The multinomial distribution models each word in a document as being drawn from a category-specific probability distribution over the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the NB model, which simply means tabulating the word counts for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(train, cat6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and predict the category labels for cat6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1., ...,  1.,  0.,  1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = mnb.predict(test);preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q3: Let's check the accuracy below. How did NB do compared to kNN? \n",
    "\n",
    "\n",
    "> ** The accuracy was the same for the kNN than the NB. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89749999999999996"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency-Based Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We argued that frequency-based feature selection is useful approach to culling features to reduce the size of a model. In some cases, accuracy improves as well. \n",
    "\n",
    "Let's construct a matrix rcounts which contains the counts of features from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3728, 4438, 3488, ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcounts = (train>0).sum(0)\n",
    "rcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll find the indices of non-zero elements of the predicate (rcounts >= count). These are the features that occurred at least count times. ii is the indices of these features.\n",
    "\n",
    "We then extract the columns of the train and test matrices corresponding to those high-count features. The last line tells us how many features we kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31331"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "threshold = 1\n",
    "ii0 = np.asarray(np.nonzero(rcounts >= threshold)[1])\n",
    "ii = ii0.reshape(ii0.size,)\n",
    "strain = train[:,ii]\n",
    "stest = test[:,ii]\n",
    "ii.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run training and evaluation on train/test data restricted to those columns to get an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Features</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>  1</td>\n",
       "      <td> 31331</td>\n",
       "      <td> 0.89750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>  2</td>\n",
       "      <td> 18330</td>\n",
       "      <td> 0.89550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>  5</td>\n",
       "      <td>  9632</td>\n",
       "      <td> 0.89000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 10</td>\n",
       "      <td>  6169</td>\n",
       "      <td> 0.88675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 20</td>\n",
       "      <td>  4046</td>\n",
       "      <td> 0.88400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count  Features    Score\n",
       "0      1     31331  0.89750\n",
       "1      2     18330  0.89550\n",
       "2      5      9632  0.89000\n",
       "3     10      6169  0.88675\n",
       "4     20      4046  0.88400"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(strain, cat6)\n",
    "preds = mnb.predict(stest);preds\n",
    "np.mean(tcat6 == preds)\n",
    "\n",
    "thresholds = [2,5,10,20]\n",
    "li = [[1, ii.size, np.mean(tcat6 == preds)]]\n",
    "for t in thresholds:\n",
    "    threshold = t\n",
    "    ii0 = np.asarray(np.nonzero(rcounts >= threshold)[1])\n",
    "    ii = ii0.reshape(ii0.size,)\n",
    "    strain = train[:,ii]\n",
    "    stest = test[:,ii]\n",
    "    ii.size\n",
    "    mnb.fit(strain, cat6)\n",
    "    preds = mnb.predict(stest);preds\n",
    "    np.mean(tcat6 == preds)\n",
    "    li.append([t, ii.size, np.mean(tcat6 == preds)])\n",
    "\n",
    "\n",
    "df3 = pd.DataFrame(li,columns=['Count','Features', 'Score'])\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q4: For threshold counts of 1, 2, 5, 10, 20, tabulate the count, number of features kept, and the accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q5: How did frequency-based feature selection affect accuracy? How much was model size (proportional to words kept) reduced? \n",
    "\n",
    ">**The more features yielded higher accuracy than lower number of features. Model size was reduced by almost half each time. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Based on Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more sophisticated method to select the (word) features. Since we have count data, the Chi-Squared test is appropriate. Remember that Chi-squared can be used on 2x2 contingency tables to determine if there is an interaction. In this case, we test if the label interacts with the feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fcounts = np.zeros((4,nwords))           # holds the counts of various combinations of label and feature\n",
    "expected = np.zeros((4,nwords))          # holds the predicted counts of those same combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mcat6 = cat6.reshape(1,ndocs)\n",
    "fcounts[0,:] = mcat6 * (train > 0)                                   # label and word both = 1\n",
    "fcounts[1,:] = (1-mcat6) * (train > 0)                               # label = 0, word = 1\n",
    "fcounts[2,:] = np.sum(mcat6) * np.ones((1,nwords)) - fcounts[0,:]    # label = 1, word = 0\n",
    "fcounts[3,:] = np.sum(1-mcat6) * np.ones((1,nwords)) - fcounts[1,:]  # label = 0, word = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the definition of the CHI-squared test here:\n",
    "https://en.wikipedia.org/wiki/Chi-squared_test\n",
    "Under the null hypothesis that category label doesn't matter, we expect to be able to compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1924.,  2273.,  2204., ...,     0.,     0.,     0.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcounts[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allf = np.sum(fcounts,axis=0)\n",
    "pword0 = (fcounts[2,:] + fcounts[3,:]) / allf    # prob word = 0\n",
    "pword1 = (fcounts[0,:] + fcounts[1,:]) / allf    # prob word = 1\n",
    "pcat0 = (fcounts[1,:] + fcounts[3,:]) / allf     # prob label = 0\n",
    "pcat1 = (fcounts[0,:] + fcounts[2,:]) / allf     # prob label = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If label and word occurence are independent, then the expected count should be the product of the total count and the corresponding probabilities:\n",
    "\n",
    "> Q6: Implement the expected counts here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected[0,:] =  pcat1 * pword1 # expected count with label = 1, word = 1\n",
    "expected[1,:] =  pcat0 * pword1  # expected count with label = 0, word = 1\n",
    "expected[2,:] =  pcat1 * pword0 # expected count with label = 1, word = 0\n",
    "expected[3,:] =  pcat0 * pword0 # expected count with label = 0, word = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the chi-squared statistic value, we want the Sum of the squared differences between actual and predicted counts, normalized by the expected counts. The following two cells perform that calculation. We have to guard against divide-by-zero so we use a small lower-bound on the denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.92382963e+03,   2.27279718e+03,   2.20384060e+03, ...,\n",
       "         -4.57000000e-05,  -4.57000000e-05,  -4.57000000e-05],\n",
       "       [  1.80379757e+03,   2.16475902e+03,   1.28381060e+03, ...,\n",
       "          9.99945700e-01,   9.99945700e-01,   9.99945700e-01],\n",
       "       [  2.64571337e+03,   2.29674582e+03,   2.36570240e+03, ...,\n",
       "          4.56954305e+03,   4.56954305e+03,   4.56954305e+03],\n",
       "       [  3.62565943e+03,   3.26469798e+03,   4.14564640e+03, ...,\n",
       "          5.42845705e+03,   5.42845705e+03,   5.42845705e+03]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = fcounts - expected\n",
    "safeexpected = np.maximum(expected,1e-7)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q7: Implement the CHI-squared statistic value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 31331)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisquaredstat =  np.square(diff) / safeexpected # implement the chisquared statistic value here\n",
    "chisquaredstat.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the chi-squared statistic to choose the best features. The CHI-squared stastistic is a measure of interaction between a word feature and the label. The higher its value, the better that word should be as a predictor of the category. We set a threshold and take the words whose chi-squared statistic is above this value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104691,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisqthreshold = 1.5\n",
    "ii0 = np.asarray(np.nonzero(chisquaredstat > chisqthreshold)[0])\n",
    "ii = ii0.reshape(ii0.size,);ii.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use the indices of good features selected by the chi-squared filter to train and evaluate the Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strain = train[:,ii]\n",
    "stest = test[:,ii]\n",
    "mnb.fit(strain, cat6)\n",
    "preds = mnb.predict(stest);preds\n",
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">(optional): Compare the accuracy of CHI-squared feature selection to the frequency-based selection you did earlier. For each frequency threshold (1,2,5,10,20), adjust the chisquared threshold to retain the smallest number of features that is at least as large as the number of features kept by that frequency threshold. Then compute the accuracy in the last cell for that threshold. Finally tabulate both frequency-based and chi-squared accuracies. That is, make a table whose columns are:\n",
    "* freq threshold\n",
    "* number of features retained by this frequency threshold\n",
    "* chisquared threshold \n",
    "* number of features retained by this chisquared threshold (should be >= column 2)\n",
    "* accuracy of frequency thresholded model\n",
    "* accuracy of chisquared thresholded model\n",
    "and whose rows are indexed by the frequency thresholds 1, 2, 5, 10, 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">(optional): Which method has better accuracy for a given number of features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your A4 notebook on Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
